{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd  /content/drive/'My Drive'/dfc2021-msd-baseline-master"
      ],
      "metadata": {
        "id": "lSUar8Z7QLwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3637e034-2549-473a-c012-a17ede502719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/dfc2021-msd-baseline-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio fiona segmentation-models-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDS-MxsxSWQz",
        "outputId": "a7b99b11-3e30-4378-d271-d90c99700b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 535 kB/s \n",
            "\u001b[?25hCollecting fiona\n",
            "  Downloading Fiona-1.8.22-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.7 MB 46.4 MB/s \n",
            "\u001b[?25hCollecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (22.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.9.24)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting affine\n",
            "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.15.0)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (4.64.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.13.1+cu113)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=8bc4306a208d2a00df04a3a3f75d9739d02324457493d404801cb5bd1f02da0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=246e40b7b6f363a3a778f1c5bcbc51eff50b6b767874d387bdfdc16d536e1cf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, snuggs, pretrainedmodels, efficientnet-pytorch, cligj, click-plugins, affine, segmentation-models-pytorch, rasterio, fiona\n",
            "Successfully installed affine-2.3.1 click-plugins-1.1.1 cligj-0.7.2 efficientnet-pytorch-0.7.1 fiona-1.8.22 munch-2.5.0 pretrainedmodels-0.7.4 rasterio-1.2.10 segmentation-models-pytorch-0.3.0 snuggs-1.4.7 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pytorch torchvision"
      ],
      "metadata": {
        "id": "MqfQWRyLS4oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --input_fn data/splits/training_set_naip_nlcd_both_sample.csv --output_dir results/unet_both_baseline/ --save_most_recent --num_epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z1op20wPjHu",
        "outputId": "ca582936-c415-4563-8846-b07aeb1e6684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DFC2021 baseline training script at 2022-11-09 00:44:34.363028\n",
            "The output directory doesn't exist or is empty.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "We will be training with 62 batches per epoch\n",
            "Model has 11780305 parameters\n",
            "64it [02:07,  1.99s/it]\n",
            "[] Training Epoch: 0\t Time elapsed: 127.37 seconds\t Loss: 1.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input_fn data/splits/val_inference_both_sample.csv --model_fn results/unet_both_baseline/most_recent_model.pt --output_dir results/unet_both_baseline/output/\n"
      ],
      "metadata": {
        "id": "m2llVJcFuqdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518c7eef-0b4e-4da3-e360-afc94717b497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DFC2021 model inference script at 2022-11-09 00:47:06.565608\n",
            "The output directory doesn't exist or is empty.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "(0/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2013/2283_naip-2013.tif ... finished in 21.3552 seconds\n",
            "(1/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2017/2283_naip-2017.tif ... finished in 19.1488 seconds\n",
            "(2/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2013/2449_naip-2013.tif ... finished in 25.0568 seconds\n",
            "(3/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2017/2449_naip-2017.tif ... finished in 21.7098 seconds\n",
            "(4/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2013/2448_naip-2013.tif ... finished in 21.3219 seconds\n",
            "(5/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2017/2448_naip-2017.tif ... finished in 28.9455 seconds\n",
            "(6/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2013/2003_naip-2013.tif ... finished in 23.6849 seconds\n",
            "(7/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2017/2003_naip-2017.tif ... finished in 22.6039 seconds\n",
            "(8/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2013/2002_naip-2013.tif ... finished in 21.6074 seconds\n",
            "(9/10) Processing https://dfc2021.blob.core.windows.net/competition-data/naip-2017/2002_naip-2017.tif ... finished in 23.3986 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python independent_pairs_to_predictions.py --input_dir results/unet_both_baseline/output/ --output_dir results/unet_both_baseline/submission/\n"
      ],
      "metadata": {
        "id": "Bs3wU2gmcXN4",
        "outputId": "2461ce8b-d1e1-4ea7-f8ed-5103405af04d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to combine predictions at 2022-11-09 00:51:04.334534\n",
            "The output directory doesn't exist or is empty.\n",
            "(0/5) Processing tile 2283 ... finished in 0.3761 seconds\n",
            "(1/5) Processing tile 2449 ... finished in 0.4408 seconds\n",
            "(2/5) Processing tile 2448 ... finished in 0.4050 seconds\n",
            "(3/5) Processing tile 2003 ... finished in 0.4055 seconds\n",
            "(4/5) Processing tile 2002 ... finished in 0.4047 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTBuqqaIfjLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntiNoiZLpxsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NICFI"
      ],
      "metadata": {
        "id": "nXdqM6Xw8jkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --input_fn data/splits/data_both_sample.csv --output_dir results/unet_both_baseline_nicfi/ --save_most_recent --num_epochs 1"
      ],
      "metadata": {
        "id": "dpgHAPbHpxu3",
        "outputId": "8b76890f-0d8a-4d98-fa58-359c7a08cbc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DFC2021 baseline training script at 2022-11-15 05:03:48.417859\n",
            "The output directory doesn't exist or is empty.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "We will be training with 25 batches per epoch\n",
            "Model has 11780305 parameters\n",
            "  0% 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "  0% 0/25 [00:02<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 164, in main\n",
            "    epoch,\n",
            "  File \"/content/drive/MyDrive/dfc2021-msd-baseline-master/utils.py\", line 143, in fit\n",
            "    outputs = model(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py\", line 29, in forward\n",
            "    features = self.encoder(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/encoders/resnet.py\", line 62, in forward\n",
            "    x = stages[i](x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 457, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 454, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "RuntimeError: Given groups=1, weight of size [64, 4, 7, 7], expected input[32, 3, 256, 256] to have 4 channels, but got 3 channels instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --input_fn data/splits/val_inference_both_sample.csv --model_fn results/unet_both_baseline_nicfi/most_recent_model.pt --output_dir results/unet_both_baseline_nicfi/output/\n"
      ],
      "metadata": {
        "id": "7Z7ekH2-p7f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img= '/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_0.jpeg'\n",
        "with rasterio.open(img) as f:\n",
        "  data = f.read()\n",
        "\n",
        "data.shape"
      ],
      "metadata": {
        "id": "hrelF1uTJ2rg",
        "outputId": "5d8f9622-9ba5-40fb-c6da-8da1c3f1c004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3899, 3897)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio\n",
        "import rasterio"
      ],
      "metadata": {
        "id": "lXoSp3qnKNPb",
        "outputId": "a32af445-aacf-4477-e3ea-d9c2b66ae74f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.7/dist-packages (1.2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.7/dist-packages (from rasterio) (2.3.1)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.9.24)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (22.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wtXAqsTKQV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([[1,0],[2,1]])\n",
        "b = np.array([[1,0],[2,5]])\n",
        "\n",
        "a*b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP5QlYA7R8VY",
        "outputId": "ea6ffb8e-31f6-43ff-c883-7eb837476873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmyrKGXhSElE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_-KjjuLs25F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checking Dataloader\n"
      ],
      "metadata": {
        "id": "jH6yn7-hs3pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from rasterio.errors import RasterioError, RasterioIOError\n",
        "\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "\n",
        "class StreamingGeospatialDataset(IterableDataset):\n",
        "    \n",
        "    def __init__(self, imagery_fns, label_fns=None, groups=None, chip_size=256, num_chips_per_tile=200, windowed_sampling=False, image_transform=None, label_transform=None, nodata_check=None, verbose=False):\n",
        "        \"\"\"A torch Dataset for randomly sampling chips from a list of tiles. When used in conjunction with a DataLoader that has `num_workers>1` this Dataset will assign each worker to sample chips from disjoint sets of tiles.\n",
        "\n",
        "        Args:\n",
        "            imagery_fns: A list of filenames (or URLS -- anything that `rasterio.open()` can read) pointing to imagery tiles.\n",
        "            label_fns: A list of filenames of the same size as `imagery_fns` pointing to label mask tiles or `None` if the Dataset should operate in \"imagery only mode\". Note that we expect `imagery_fns[i]` and `label_fns[i]` to have the same dimension and coordinate system.\n",
        "            groups: Optional: A list of integers of the same size as `imagery_fns` that gives the \"group\" membership of each tile. This can be used to normalize imagery from different groups differently.\n",
        "            chip_size: Desired size of chips (in pixels).\n",
        "            num_chips_per_tile: Desired number of chips to sample for each tile.\n",
        "            windowed_sampling: Flag indicating whether we should sample each chip with a read using `rasterio.windows.Window` or whether we should read the whole tile into memory, then sample chips.\n",
        "            image_transform: A function to apply to each image chip object. If this is `None`, then the only transformation applied to the loaded imagery will be to convert it to a `torch.Tensor`. If this is not `None`, then the function should return a `Torch.tensor`. Further, if `groups` is not `None` then the transform function should expect the imagery as the first argument and the group as the second argument.\n",
        "            label_transform: Similar to image_transform, but applied to label chips.\n",
        "            nodata_check: A method that will check an `(image_chip)` or `(image_chip, label_chip)` (if `label_fns` are provided) and return whether or not the chip should be skipped. This can be used, for example, to skip chips that contain nodata values.\n",
        "            verbose: If `False` we will be quiet.\n",
        "        \"\"\"\n",
        "\n",
        "        if label_fns is None:\n",
        "            self.fns = imagery_fns\n",
        "            self.use_labels = False\n",
        "        else:\n",
        "            self.fns = list(zip(imagery_fns, label_fns)) \n",
        "            self.use_labels = True\n",
        "\n",
        "        self.groups = groups\n",
        "\n",
        "        self.chip_size = chip_size\n",
        "        self.num_chips_per_tile = num_chips_per_tile\n",
        "        self.windowed_sampling = windowed_sampling\n",
        "\n",
        "        self.image_transform = image_transform\n",
        "        self.label_transform = label_transform\n",
        "        self.nodata_check = nodata_check\n",
        "\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Constructed StreamingGeospatialDataset\")\n",
        "\n",
        "    def stream_tile_fns(self):\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        if worker_info is None: # In this case we are not loading through a DataLoader with multiple workers\n",
        "            worker_id = 0\n",
        "            num_workers = 1\n",
        "        else:\n",
        "            worker_id = worker_info.id\n",
        "            num_workers = worker_info.num_workers\n",
        "\n",
        "        # We only want to shuffle the order we traverse the files if we are the first worker (else, every worker will shuffle the files...)\n",
        "        if worker_id == 0:\n",
        "            np.random.shuffle(self.fns) # in place\n",
        "        # NOTE: A warning, when different workers are created they will all have the same numpy random seed, however will have different torch random seeds. If you want to use numpy random functions, seed appropriately.\n",
        "        #seed = torch.randint(low=0,high=2**32-1,size=(1,)).item()\n",
        "        #np.random.seed(seed) # when different workers spawn, they have the same numpy random seed...\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Creating a filename stream for worker %d\" % (worker_id))\n",
        "\n",
        "        # This logic splits up the list of filenames into `num_workers` chunks. Each worker will recieve ceil(num_filenames / num_workers) filenames to generate chips from. If the number of workers doesn't divide the number of filenames evenly then the last worker will have fewer filenames.\n",
        "        N = len(self.fns)\n",
        "        num_files_per_worker = int(np.ceil(N / num_workers))\n",
        "        lower_idx = worker_id * num_files_per_worker\n",
        "        upper_idx = min(N, (worker_id+1) * num_files_per_worker)\n",
        "        for idx in range(lower_idx, upper_idx):\n",
        "\n",
        "            label_fn = None\n",
        "            if self.use_labels:\n",
        "                img_fn, label_fn = self.fns[idx]\n",
        "            else:\n",
        "                img_fn = self.fns[idx]\n",
        "\n",
        "            if self.groups is not None:\n",
        "                group = self.groups[idx]\n",
        "            else:\n",
        "                group = None\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"Worker %d, yielding file %d\" % (worker_id, idx))\n",
        "\n",
        "            yield (img_fn, label_fn, group)\n",
        "\n",
        "    def stream_chips(self):\n",
        "        for img_fn, label_fn, group in self.stream_tile_fns():\n",
        "            num_skipped_chips = 0\n",
        "\n",
        "            # Open file pointers\n",
        "            img_fp = rasterio.open(img_fn, \"r\")\n",
        "            label_fp = rasterio.open(label_fn, \"r\") if self.use_labels else None\n",
        "\n",
        "            height, width = img_fp.shape\n",
        "            if self.use_labels: # garuntee that our label mask has the same dimensions as our imagery\n",
        "                t_height, t_width = label_fp.shape\n",
        "                assert height == t_height and width == t_width\n",
        "\n",
        "\n",
        "            # If we aren't in windowed sampling mode then we should read the entire tile up front\n",
        "            img_data = None\n",
        "            label_data = None\n",
        "            try:\n",
        "                if not self.windowed_sampling:\n",
        "                    img_data = np.rollaxis(img_fp.read(), 0, 3)\n",
        "                    if self.use_labels:\n",
        "                        label_data = label_fp.read().squeeze() # assume the label geotiff has a single channel\n",
        "            except RasterioError as e:\n",
        "                print(\"WARNING: Error reading in entire file, skipping to the next file\")\n",
        "                continue\n",
        "\n",
        "            for i in range(self.num_chips_per_tile):\n",
        "                # Select the top left pixel of our chip randomly\n",
        "                x = np.random.randint(0, width-self.chip_size)\n",
        "                y = np.random.randint(0, height-self.chip_size)\n",
        "\n",
        "                # Read imagery / labels\n",
        "                img = None\n",
        "                labels = None\n",
        "                if self.windowed_sampling:\n",
        "                    try:\n",
        "                        img = np.rollaxis(img_fp.read(window=Window(x, y, self.chip_size, self.chip_size)), 0, 3)\n",
        "                        print(img.shape)\n",
        "                        if self.use_labels:\n",
        "                            labels = label_fp.read(window=Window(x, y, self.chip_size, self.chip_size)).squeeze()\n",
        "                    except RasterioError:\n",
        "                        print(\"WARNING: Error reading chip from file, skipping to the next chip\")\n",
        "                        continue\n",
        "                else:\n",
        "                    img = img_data[y:y+self.chip_size, x:x+self.chip_size, :]\n",
        "                    if self.use_labels:\n",
        "                        labels = label_data[y:y+self.chip_size, x:x+self.chip_size]\n",
        "\n",
        "                # Check for no data\n",
        "                if self.nodata_check is not None:\n",
        "                    if self.use_labels:\n",
        "                        skip_chip = self.nodata_check(img, labels)\n",
        "                    else:\n",
        "                        skip_chip = self.nodata_check(img)\n",
        "\n",
        "                    if skip_chip: # The current chip has been identified as invalid by the `nodata_check(...)` method\n",
        "                        num_skipped_chips += 1\n",
        "                        continue\n",
        "\n",
        "                # Transform the imagery\n",
        "                if self.image_transform is not None:\n",
        "                    if self.groups is None:\n",
        "                        img = self.image_transform(img)\n",
        "                    else:\n",
        "                        img = self.image_transform(img, group)\n",
        "                else:\n",
        "                    img = torch.from_numpy(img).squeeze()\n",
        "\n",
        "                # Transform the labels\n",
        "                if self.use_labels:\n",
        "                    if self.label_transform is not None:\n",
        "                        if self.groups is None:\n",
        "                            labels = self.label_transform(labels)\n",
        "                        else:\n",
        "                            labels = self.label_transform(labels, group)\n",
        "                    else:\n",
        "                        labels = torch.from_numpy(labels).squeeze()\n",
        "\n",
        "\n",
        "                # Note, that img should be a torch \"Double\" type (i.e. a np.float32) and labels should be a torch \"Long\" type (i.e. np.int64)\n",
        "                if self.use_labels:\n",
        "                    yield img, labels\n",
        "                else:\n",
        "                    yield img\n",
        "\n",
        "            # Close file pointers\n",
        "            img_fp.close()\n",
        "            if self.use_labels:\n",
        "                label_fp.close()\n",
        "\n",
        "            if num_skipped_chips>0 and self.verbose:\n",
        "                print(\"We skipped %d chips on %s\" % (img_fn))\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.verbose:\n",
        "            print(\"Creating a new StreamingGeospatialDataset iterator\")\n",
        "        return iter(self.stream_chips())\n"
      ],
      "metadata": {
        "id": "FQAwV40ks28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NAIP_2013_MEANS = np.array([83.18, 79.00, 55.65])\n",
        "NAIP_2013_STDS = np.array([13.83,  9.16, 10.09])\n",
        "NAIP_2017_MEANS = np.array([80.80, 79.22, 54.69])\n",
        "NAIP_2017_STDS = np.array([9.35 , 5.94, 6.79])\n",
        "\n",
        "def image_transforms(img, group):\n",
        "    if group == 0:\n",
        "        img = (img - NAIP_2013_MEANS) / NAIP_2013_STDS\n",
        "    elif group == 1:\n",
        "        img = (img - NAIP_2017_MEANS) / NAIP_2017_STDS\n",
        "    else:\n",
        "        raise ValueError(\"group not recognized\")\n",
        "    img = np.rollaxis(img, 2, 0).astype(np.float32)\n",
        "    img = torch.from_numpy(img)\n",
        "    return img\n",
        "\n",
        "def label_transforms(labels, group):\n",
        "    labels = utils.NLCD_CLASS_TO_IDX_MAP[labels]\n",
        "    labels = torch.from_numpy(labels)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "zXT2oYSsv5mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_dataframe = pd.read_csv('data/splits/data_both_sample.csv')\n",
        "image_fns = input_dataframe[\"image_fn\"].values\n",
        "label_fns = input_dataframe[\"label_fn\"].values\n",
        "groups = input_dataframe[\"group\"].values\n",
        "\n",
        "training_set_naip_nlcd_both_sample"
      ],
      "metadata": {
        "id": "DSjEIfzWtCfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_fns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgXobH5tuEZg",
        "outputId": "a567a1fd-1ec9-4a27-9a10-366feb267c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/drive/MyDrive/GEE_regen/00.tif',\n",
              "       '/content/drive/MyDrive/GEE_2019_final/00.tif',\n",
              "       '/content/drive/MyDrive/GEE_regen/02.tif',\n",
              "       '/content/drive/MyDrive/GEE_2019_final/01.tif',\n",
              "       '/content/drive/MyDrive/GEE_regen/01.tif',\n",
              "       '/content/drive/MyDrive/GEE_2019_final/04.tif',\n",
              "       '/content/drive/MyDrive/GEE_regen/03.tif',\n",
              "       '/content/drive/MyDrive/GEE_2019_final/03.tif'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_fns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqFOGqnfuIWD",
        "outputId": "1c430540-1882-4a27-bf48-8ac7846f7a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_0.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2019_three_class/0_0.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_2.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2019_three_class/0_1.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_1.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2019_three_class/0_4.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_3.jpeg',\n",
              "       '/content/drive/MyDrive/GEE_Dynamic_2019_three_class/0_3.jpeg'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StreamingGeospatialDataset(\n",
        "        imagery_fns=image_fns, label_fns=label_fns, groups=groups, chip_size=256, num_chips_per_tile=100, windowed_sampling=False, verbose=False,\n",
        "        image_transform=image_transforms, label_transform=None, nodata_check=None\n",
        "    )"
      ],
      "metadata": {
        "id": "nva3jInYuYc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "-BzxL4cVvHDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTuH7BqsvH-h",
        "outputId": "e1476932-ec43-4a86-95bb-65dd836bac89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f35d1c7d210>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import models\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import utils\n",
        "\n",
        "num_training_batches_per_epoch = int(len(image_fns) * 100 / 32)\n",
        "print(\"We will be training with %d batches per epoch\" % (num_training_batches_per_epoch))\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\", 0)\n",
        "\n",
        "model = models.get_unet()\n",
        "\n",
        "\n",
        "model = smp.Unet(\n",
        "        encoder_name='resnet18', encoder_depth=3, encoder_weights=None,\n",
        "        decoder_channels=(128, 64, 64), in_channels=4, classes=16)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Haj-aPvk53",
        "outputId": "5ff075ab-2eb9-4bd8-b80a-41710c9306f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We will be training with 62 batches per epoch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_task_losses = []\n",
        "num_times_lr_dropped = 0 \n",
        "model_checkpoints = []\n",
        "temp_model_fn = os.path.join('results/unet_both_baseline_nicfi/', \"most_recent_model.pt\")\n",
        "\n",
        "for epoch in range(1):\n",
        "    lr = utils.get_lr(optimizer)\n",
        "\n",
        "    training_losses = utils.fit(\n",
        "        model,\n",
        "        device,\n",
        "        dataloader,\n",
        "        num_training_batches_per_epoch,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epoch,\n",
        "    )\n",
        "    scheduler.step(training_losses[0])\n",
        "\n",
        "    model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "    #if args.save_most_recent:\n",
        "    torch.save(model.state_dict(), temp_model_fn)\n",
        "\n",
        "    if utils.get_lr(optimizer) < lr:\n",
        "        num_times_lr_dropped += 1\n",
        "        print(\"\")\n",
        "        print(\"Learning rate dropped\")\n",
        "        print(\"\")\n",
        "        \n",
        "    training_task_losses.append(training_losses[0])\n",
        "        \n",
        "    if num_times_lr_dropped == 4:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "i80nYwRDxO6C",
        "outputId": "725d9991-c2f0-4816-a23e-3b805a561f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/25 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b6f9a7f7d939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/dfc2021-msd-baseline-master/utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, device, data_loader, num_batches, optimizer, criterion, epoch, memo)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/encoders/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 4, 7, 7], expected input[32, 3, 256, 256] to have 4 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_fp = rasterio.open('/content/drive/MyDrive/GEE_regen/00.tif', \"r\")\n",
        "height, width = img_fp.shape\n",
        "print(height, width)\n",
        "\n",
        "label_fp = rasterio.open('/content/drive/MyDrive/GEE_Dynamic_2016_three_class/0_0.jpeg', \"r\")\n",
        "#label_fp = rasterio.open('/content/drive/MyDrive/GEE_Dynamic_2016_three_class_np_label/0_0.npy', \"r\")\n",
        "t_height, t_width = label_fp.shape\n",
        "print(t_height, t_width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MplF6XGzyf91",
        "outputId": "4b65f2ec-2cf8-4017-e447-bf178b7d9a1f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3899 3897\n",
            "3899 3897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rasterio/__init__.py:220: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
            "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_data = np.rollaxis(img_fp.read(), 0, 3)\n",
        "\n",
        "label_data = label_fp.read().squeeze() # assume the label geotiff has a single channel\n",
        "\n",
        "label_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QohTCo920oM4",
        "outputId": "38a72154-40d5-4f34-d08f-5460a3db504d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3899, 3897)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyFK5xh311uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "img_fp = rasterio.open('https://dfc2021.blob.core.windows.net/competition-data/naip-2013/546_naip-2013.tif', \"r\")\n",
        "height, width = img_fp.shape\n",
        "print(height, width)\n",
        "\n",
        "label_fp = rasterio.open('https://dfc2021.blob.core.windows.net/competition-data/nlcd-2013/546_nlcd-2013.tif', \"r\")\n",
        "t_height, t_width = label_fp.shape\n",
        "print(t_height, t_width)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBTreqBp2V3-",
        "outputId": "1e6a3bf8-c040-4625-c51e-6dd6d5ffa4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3880 3880\n",
            "3880 3880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "img_data = np.rollaxis(img_fp.read(), 0, 3)\n",
        "\n",
        "label_data = label_fp.read().squeeze() # assume the label geotiff has a single channel\n",
        "\n",
        "label_data.shape\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XgcXEwR2dlp",
        "outputId": "d6d3c585-fd02-4404-e97e-5e7c5f29118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3880, 3880)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = img_data[0:256, 0:256, :]\n",
        "\n",
        "labels = label_data[0:256, 0:256]\n",
        "\n",
        "\n",
        "NAIP_2013_MEANS = np.array([117.00, 130.75, 122.50, 159.30])\n",
        "NAIP_2013_STDS = np.array([38.16, 36.68, 24.30, 66.22])\n",
        "NAIP_2017_MEANS = np.array([72.84,  86.83, 76.78, 130.82])\n",
        "NAIP_2017_STDS = np.array([41.78, 34.66, 28.76, 58.95])\n",
        "\n",
        "img=image_transforms(img,0)\n",
        "labels = label_transforms(labels, 0)"
      ],
      "metadata": {
        "id": "oknrHg992wtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9jSOwf8UAK",
        "outputId": "09c37603-753a-4f3c-efd0-543c4844b9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10, 10, 10,  ...,  8,  8,  8],\n",
              "        [10, 10, 10,  ...,  8,  8,  8],\n",
              "        [ 3,  3, 10,  ...,  8,  8,  8],\n",
              "        ...,\n",
              "        [ 8,  8,  8,  ...,  8,  8,  8],\n",
              "        [ 8,  8,  8,  ...,  8,  8,  8],\n",
              "        [ 8,  8,  8,  ...,  8,  8,  8]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpORDL4l8cJv",
        "outputId": "31443ad8-b263-4037-db37-06a18a526845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 43, 43, ..., 81, 81, 81], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLCD_CLASSES = [ 0, 11, 12, 21, 22, 23, 24, 31, 41, 42, 43, 52, 71, 81, 82, 90, 95]\n",
        "\n",
        "def get_nlcd_class_to_idx_map():\n",
        "    nlcd_label_to_idx_map = []\n",
        "    idx = 0\n",
        "    for i in range(NLCD_CLASSES[-1]+1):\n",
        "        if i in NLCD_CLASSES:\n",
        "            nlcd_label_to_idx_map.append(idx)\n",
        "            idx += 1\n",
        "        else:\n",
        "            nlcd_label_to_idx_map.append(0)\n",
        "    nlcd_label_to_idx_map = np.array(nlcd_label_to_idx_map).astype(np.int64)\n",
        "    return nlcd_label_to_idx_map\n",
        "\n",
        "NLCD_CLASS_TO_IDX_MAP = get_nlcd_class_to_idx_map()\n",
        "\n",
        "def label_transforms(labels, group):\n",
        "    labels = NLCD_CLASS_TO_IDX_MAP[labels]\n",
        "    labels = torch.from_numpy(labels)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "LiXL13Vf22yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_transforms(labels, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "B6814iG56u5D",
        "outputId": "b07956e2-6926-4995-d4a3-1e3283fafc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-718dd9b6e38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-a6d3585e9de1>\u001b[0m in \u001b[0;36mlabel_transforms\u001b[0;34m(labels, group)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlabel_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLCD_CLASS_TO_IDX_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 57 is out of bounds for axis 0 with size 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLCD_CLASS_TO_IDX_MAP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtoInqO26yHP",
        "outputId": "b1df9190-f886-4b29-d56d-1134d09ca132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  3,  4,  5,  6,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  8,  9, 10,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13, 14,  0,  0,\n",
              "        0,  0,  0,  0,  0, 15,  0,  0,  0,  0, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hp = {}\n",
        "for i in label_data[0]:\n",
        "  if i in hp:\n",
        "    hp[i]+=1\n",
        "  else:\n",
        "    hp[i]=1\n",
        "\n",
        "hp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5Rx6yb7YGL",
        "outputId": "503e708e-8345-4470-9991-5907b36b1780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{43: 122, 21: 93, 41: 2773, 52: 62, 71: 711, 81: 88, 31: 31}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hp = {}\n",
        "for i in label_data[0][0]:\n",
        "  if i in hp:\n",
        "    hp[i]+=1\n",
        "  else:\n",
        "    hp[i]=1\n",
        "\n",
        "hp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZcnTtqW8__3",
        "outputId": "99f272f9-41cc-4a12-f01d-a0e1f4955876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{57: 2565,\n",
              " 50: 6,\n",
              " 58: 7,\n",
              " 69: 4,\n",
              " 60: 5,\n",
              " 42: 7,\n",
              " 55: 5,\n",
              " 98: 2,\n",
              " 144: 2,\n",
              " 193: 43,\n",
              " 210: 7,\n",
              " 204: 8,\n",
              " 197: 12,\n",
              " 199: 8,\n",
              " 196: 847,\n",
              " 190: 8,\n",
              " 225: 1,\n",
              " 224: 2,\n",
              " 222: 4,\n",
              " 221: 1,\n",
              " 215: 2,\n",
              " 208: 10,\n",
              " 186: 10,\n",
              " 171: 2,\n",
              " 156: 1,\n",
              " 141: 1,\n",
              " 129: 1,\n",
              " 118: 1,\n",
              " 110: 2,\n",
              " 107: 2,\n",
              " 111: 1,\n",
              " 121: 1,\n",
              " 128: 1,\n",
              " 63: 68,\n",
              " 23: 1,\n",
              " 26: 1,\n",
              " 65: 7,\n",
              " 93: 3,\n",
              " 85: 2,\n",
              " 68: 10,\n",
              " 95: 1,\n",
              " 27: 3,\n",
              " 37: 1,\n",
              " 61: 4,\n",
              " 71: 3,\n",
              " 38: 2,\n",
              " 46: 1,\n",
              " 97: 2,\n",
              " 167: 1,\n",
              " 219: 1,\n",
              " 243: 1,\n",
              " 182: 2,\n",
              " 200: 9,\n",
              " 183: 4,\n",
              " 179: 6,\n",
              " 192: 3,\n",
              " 207: 4,\n",
              " 206: 5,\n",
              " 214: 6,\n",
              " 189: 3,\n",
              " 162: 1,\n",
              " 178: 4,\n",
              " 201: 9,\n",
              " 213: 4,\n",
              " 203: 3,\n",
              " 194: 8,\n",
              " 230: 1,\n",
              " 174: 3,\n",
              " 106: 1,\n",
              " 43: 4,\n",
              " 79: 2,\n",
              " 56: 3,\n",
              " 49: 4,\n",
              " 54: 5,\n",
              " 36: 3,\n",
              " 53: 3,\n",
              " 78: 2,\n",
              " 74: 3,\n",
              " 47: 7,\n",
              " 45: 3,\n",
              " 90: 2,\n",
              " 155: 2,\n",
              " 217: 5,\n",
              " 187: 5,\n",
              " 202: 4,\n",
              " 226: 3,\n",
              " 109: 2,\n",
              " 35: 2,\n",
              " 185: 2,\n",
              " 176: 2,\n",
              " 209: 1,\n",
              " 164: 3,\n",
              " 67: 1,\n",
              " 99: 1,\n",
              " 168: 2,\n",
              " 205: 2,\n",
              " 231: 1,\n",
              " 102: 3,\n",
              " 25: 1,\n",
              " 70: 3,\n",
              " 80: 2,\n",
              " 77: 1,\n",
              " 91: 1,\n",
              " 89: 2,\n",
              " 29: 2,\n",
              " 170: 1,\n",
              " 140: 1,\n",
              " 211: 2,\n",
              " 218: 1,\n",
              " 184: 1,\n",
              " 108: 1,\n",
              " 40: 1,\n",
              " 81: 3,\n",
              " 32: 2,\n",
              " 39: 1,\n",
              " 105: 1,\n",
              " 172: 2,\n",
              " 229: 1,\n",
              " 22: 1,\n",
              " 76: 1,\n",
              " 48: 2,\n",
              " 41: 2,\n",
              " 160: 2,\n",
              " 198: 2,\n",
              " 191: 2,\n",
              " 51: 2,\n",
              " 62: 1,\n",
              " 92: 1,\n",
              " 152: 1,\n",
              " 188: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DweVOZk9nu2",
        "outputId": "a52e9c83-e8aa-4024-ca2c-facac68f3b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 57,  57,  57, ..., 196, 196, 196],\n",
              "       [ 57,  57,  57, ..., 196, 196, 196],\n",
              "       [ 57,  57,  57, ..., 196, 196, 196],\n",
              "       ...,\n",
              "       [ 57,  57,  57, ...,  58,  58,  58],\n",
              "       [ 57,  57,  57, ...,  47,  47,  47],\n",
              "       [ 57,  57,  57, ...,  51,  51,  51]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aejyimJp-Khw",
        "outputId": "94111509-9abe-4ad7-c78a-a07d18983998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[53, 62, 39],\n",
              "       [51, 61, 39],\n",
              "       [51, 61, 39],\n",
              "       ...,\n",
              "       [59, 63, 44],\n",
              "       [59, 64, 44],\n",
              "       [59, 64, 44]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYubN3BVAP_8",
        "outputId": "03409e4b-62b7-4932-b90a-bc9bb0981891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3899, 3897)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLCD"
      ],
      "metadata": {
        "id": "4YqQoCJR-VQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcPe09bO9qyQ",
        "outputId": "dcae7360-d90e-408e-f97a-152bc4014c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 43, 43, ..., 81, 81, 81], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5O9oC2O-K-W",
        "outputId": "a9f40399-252d-4272-c4ef-cd8c74dba792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 81,  81,  95, 173],\n",
              "       [ 82,  86,  95, 178],\n",
              "       [ 81,  87,  95, 183],\n",
              "       ...,\n",
              "       [112, 126, 117, 224],\n",
              "       [102, 118, 114, 224],\n",
              "       [109, 130, 116, 226]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_data[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDL2lT-S-D1w",
        "outputId": "621737b6-da13-4740-892b-6c35d58afaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 57,  57,  57, ..., 196, 196, 196], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DH5VKXEr-hAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}